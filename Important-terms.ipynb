{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>An activation function is a function that maps a node's inputs to its corresponding output.\n",
    "    \n",
    "<p> The weighted sum of each incoming connection for each node in the layer is taken, and pass that weighted sum to an activation function.</p>\n",
    "\n",
    "<p style=\"text-align:center; border: 2px solid red;\">node output = activation(weighted sum of inputs)</p>\n",
    "\n",
    "The activation function does some type of operation to transform the sum to a number that is often times between some lower limit and some upper limit. This transformation is often a non-linear transformation. Some of the activations functions are:\n",
    "\n",
    "\n",
    "</p>\n",
    "\n",
    "<li>Sigmoid</li>\n",
    "<li>tanh</li>\n",
    "<li>ReLU</li>\n",
    "\n",
    "<h4>ReLU</h4>\n",
    "<p>ReLU, which is short for rectified linear unit, transforms the input to the maximum of either zero or the input itself.\n",
    "\n",
    "<p style=\"text-align:center; border: 2px solid red;\">ReLU(x) = max(0, x)</p>\n",
    "\n",
    "<img src=\"relu-graph.png\" style=\"height:300px;width:600px;align:center\" title=\"ReLU Graph\"/>\n",
    "\n",
    "If the input is less than or equal to zero, then relu will output zero. If the input is greater than zero, relu will then just output the given input.<br>\n",
    "The idea here is, the more positive the neuron is, the more activated it is.\n",
    "\n",
    "An important feature of linear functions is that the composition of two linear functions is also a linear function. This means that, even in very deep neural networks, if we only had linear transformations of our data values during a forward pass, the learned mapping in our network from input to output would also be linear.\n",
    "\n",
    "Typically, the types of mappings that we are aiming to learn with our deep neural networks are more complex than simple linear mappings.\n",
    "\n",
    "This is where activation functions come in. Most activation functions are non-linear, and they are chosen in this way on purpose. Having non-linear activation functions allows our neural networks to compute arbitrarily complex functions.\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The loss function is what SGD is attempting to minimize by iteratively updating the weights in the network.\n",
    "\n",
    "At the end of each epoch during the training process, the loss will be calculated using the networkâ€™s output predictions and the true labels for the respective input.\n",
    "</p>\n",
    "\n",
    "<p>Some of the loss functions are: \n",
    "<li>mean_squared_error</li>\n",
    "<li>mean_absolute_error</li>\n",
    "<li>categorical_hinge</li>\n",
    "<li>logcosh</li>\n",
    "<li>categorical_crossentropy</li>\n",
    "<li>sparse_categorical_crossentropy</li>\n",
    "<li>binary_crossentropy</li>\n",
    "    \n",
    "</p>\n",
    "\n",
    "<h4>categorical_crossentropy</h4>\n",
    "<p>\n",
    "Mathematically,<br>\n",
    "<img src=\"crossEntropy.png\" style=\"align:center\" title=\"Categorical cross entrophy\"/><br>\n",
    "where p  are the predictions, t are the targets, i denotes the data point and j denotes the class.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Gradient descent is probably the most popular and widely used out of all optimizers. \n",
    "\n",
    "It is a simple and effective method to find the optimum values for the neural network. The objective of all optimizers is to reach the global minima where the cost function attains the least possible value. If you try to visualize the cost function in three-dimension it would something like the figure shown below.\n",
    "    \n",
    "<img src=\"gd.jpg\" style=\"align:center\" title=\"Gradient Descent\"/><br>\n",
    "    \n",
    "Some of the Gradient Descent Optimizers are:\n",
    "<li>Momentum Optimization</li>\n",
    "<li>RMSProp</li>\n",
    "<li>Adam</li>\n",
    "    \n",
    "<h4>Adam</h4>\n",
    "<p>Combination of Momentum Optimization and RMSProp</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sources</h3>\n",
    "<p>\n",
    "   \n",
    "<li>Siraj Rival (Youtube)</li>\n",
    "<a href=\"https://www.youtube.com/watch?v=FTr3n7uBIuE&t=1s\" style=\"margin-left:30px;\">Convolution Neural Network</a><br>\n",
    "<a href=\"https://www.youtube.com/watch?v=-7scQpJT7uo&t=1s\" style=\"margin-left:30px;\">Activation Functions</a> <br>\n",
    "<a href=\"https://www.youtube.com/watch?v=IVVVjBSk9N0\" style=\"margin-left:30px;\">Loss Functions</a>\n",
    "    \n",
    "<li><a href=\"https://www.youtube.com/watch?v=JXQT_vxqwIs&t=1s\">Deeplearning.ai (Youtube)</a></li>\n",
    "    \n",
    "<li><a href=\"https://www.youtube.com/watch?v=umGJ30-15_A&t=1s\">edureka!</a></li>\n",
    "    \n",
    "<li><a href=\"https://www.youtube.com/playlist?list=PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU\">deeplizard</a></li>\n",
    "\n",
    "\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
